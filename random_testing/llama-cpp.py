import llama_cpp

# Path to the LLaMA model file (the model needs to be downloaded separately)
model_path = "models/qwen2.5-7b-instruct-q2_k.gguf"

# Initialize the Llama model
llama = llama_cpp.Llama(model_path=model_path)

# Prepare a prompt to run inference on
prompt = "Once upon a time, in a land far away,"

# Perform inference
output = llama.generate(prompt)

# Print the output text generated by the model
print(output)